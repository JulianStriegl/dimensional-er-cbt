{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "outputs": [],
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-YbjCkzw0yU",
    "outputId": "5fd7c4e6-efc1-4d16-dd11-6028d90b0641"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wordcloud\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WD-FFVu2JsW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHCf-kz52BVr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ML-based Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3v6-Uk62HyA"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtRV0thhLrFT"
   },
   "source": [
    "#### Importing, extracting, normalising EmoBank, GoEmotion & ISEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOSlVHdjqW92"
   },
   "source": [
    "EmoBank Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSvFzBVBu6_7",
    "outputId": "de1897fe-993c-48c2-e91e-e90dd621633b"
   },
   "outputs": [],
   "source": [
    "train_emo_ds = pd.read_csv(\"content/Emotion-Detection-Datasets/emobank.csv\",names=[\"id\", \"split\", \"V\", \"A\", \"D\",\"text\"],skiprows=1)\n",
    "\n",
    "def normalize(data):\n",
    "  return (data-1)/4\n",
    "\n",
    "# split dataset into v,a,d\n",
    "emo_data = np.array(train_emo_ds.pop('text'))\n",
    "emo_v = normalize(np.array(train_emo_ds.pop('V')))\n",
    "emo_a = normalize(np.array(train_emo_ds.pop('A')))\n",
    "emo_d = normalize(np.array(train_emo_ds.pop('D')))\n",
    "emo_vad = np.array([emo_v,emo_a,emo_d]).T\n",
    "print(emo_vad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS7jdlFbWvat"
   },
   "source": [
    "GoEmotion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbE23TOjWuYo",
    "outputId": "636d50b3-71e9-49ac-e8b9-33cc59b88f22"
   },
   "outputs": [],
   "source": [
    "lexicon_path = \"content/Emotion-Detection-Datasets/NRC-VAD-Lexicon.txt\"\n",
    "lex = {}\n",
    "\n",
    "with open(lexicon_path) as f:\n",
    "  for line in f:\n",
    "    words = line.split()\n",
    "    try:\n",
    "      lex[words[0]] = [float(words[1]),float(words[2]),float(words[3])]\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "raw_data = pd.read_csv(\"content/Emotion-Detection-Datasets/goemotions.csv\")\n",
    "data_text = raw_data[[\"text\"]]\n",
    "data_scores = raw_data[[\"admiration\",\"amusement\",\"anger\",\n",
    "                          \"annoyance\",\"approval\",\"caring\",\"confusion\",\n",
    "                          \"curiosity\",\"desire\",\"disappointment\",\"disapproval\",\n",
    "                          \"disgust\",\"embarrassment\",\"excitement\",\"fear\",\"gratitude\",\n",
    "                          \"grief\",\"joy\",\"love\",\"nervousness\",\"optimism\",\"pride\",\n",
    "                          \"realization\",\"relief\",\"remorse\",\"sadness\",\"surprise\",\"neutral\"]]\n",
    "\n",
    "train_text = []\n",
    "labels = [\"admiration\",\"amusement\",\"anger\",\n",
    "                          \"annoyance\",\"approval\",\"caring\",\"confusion\",\n",
    "                          \"curiosity\",\"desire\",\"disappointment\",\"disapproval\",\n",
    "                          \"disgust\",\"embarrassment\",\"excitement\",\"fear\",\"gratitude\",\n",
    "                          \"grief\",\"joy\",\"love\",\"nervousness\",\"optimism\",\"pride\",\n",
    "                          \"realization\",\"relief\",\"remorse\",\"sadness\",\"surprise\",\"neutral\"]\n",
    "text = {}\n",
    "for entry in data_text.get(\"text\"):\n",
    "  text[entry] = [0]*28\n",
    "  train_text.append(entry)\n",
    "for i in range(len(train_text)):\n",
    "  train_text[i] = [train_text[i],[0,0,0],0]\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "  emotions = []\n",
    "  for entry in data_scores:\n",
    "    emotions.append(data_scores.get(entry)[i])\n",
    "  text[data_text.get(\"text\")[i]] = np.add(text[data_text.get(\"text\")[i]],emotions)\n",
    "\n",
    "scores = {}\n",
    "for item in text:\n",
    "  scores[item] = [[0,0,0],0]\n",
    "  for x in range(28):\n",
    "    for i in range(text[item][x]):\n",
    "      scores[item] = [np.add(scores[item][0],[float(j) for j in lex[labels[x]]]),scores[item][1]+1]\n",
    "\n",
    "for item in scores:\n",
    "  try:\n",
    "    scores[item][0] = [float(x)/scores[item][1] for x in scores[item][0]]\n",
    "  except:\n",
    "    del item\n",
    "\n",
    "# prepare Goemotion data to format\n",
    "go_data = []\n",
    "go_v = []\n",
    "go_a = []\n",
    "go_d = []\n",
    "go_vad = []\n",
    "\n",
    "for sentence in text.keys():\n",
    "  go_data.append(sentence)\n",
    "  go_v.append(scores[sentence][0][0])\n",
    "  go_a.append(scores[sentence][0][1])\n",
    "  go_d.append(scores[sentence][0][2])\n",
    "  go_vad.append([scores[sentence][0][0],scores[sentence][0][1],scores[sentence][0][2]])\n",
    "\n",
    "\n",
    "go_data = np.array(go_data)\n",
    "go_v = np.array(go_v)\n",
    "go_a = np.array(go_a)\n",
    "go_d = np.array(go_d)\n",
    "go_vad = np.array(go_vad)\n",
    "\n",
    "print(go_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbXs0qpu8zQg"
   },
   "source": [
    "ISEAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQ3ay1Z58yu3",
    "outputId": "06e216ff-8afd-4012-bab5-ed81b7709e86"
   },
   "outputs": [],
   "source": [
    "lexicon_path = \"content/Emotion-Detection-Datasets/NRC-VAD-Lexicon.txt\"\n",
    "lex = {}\n",
    "\n",
    "with open(lexicon_path) as f:\n",
    "  for line in f:\n",
    "    words = line.split()\n",
    "    try:\n",
    "      lex[words[0]] = [float(words[1]),float(words[2]),float(words[3])]\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "raw_data = pd.read_csv('content/Emotion-Detection-Datasets/isear.csv', delimiter = '|', on_bad_lines='skip', encoding='ISO-8859-1')\n",
    "data_text = raw_data[[\"SIT\"]]\n",
    "data_scores = raw_data[[\"EMOT\"]]\n",
    "\n",
    "train_text = []\n",
    "\n",
    "for entry in data_text.get(\"SIT\"):\n",
    "    train_text.append(entry)\n",
    "for i in range(len(train_text)):\n",
    "  train_text[i] = [train_text[i],[0,0,0]]\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "\n",
    "  if data_scores.get(\"EMOT\")[i] == 1:\n",
    "   data_scores.get(\"EMOT\")[i] = \"joy\"\n",
    "   train_text[i][1] = [float(i) for i in lex[\"joy\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 2:\n",
    "   data_scores.get(\"EMOT\")[i] = \"fear\"\n",
    "   train_text[i][1] = [float(i) for i in lex[\"fear\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 3:\n",
    "   data_scores.get(\"EMOT\")[i] = \"anger\" \n",
    "   train_text[i][1] = [float(i) for i in lex[\"anger\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 4:\n",
    "   data_scores.get(\"EMOT\")[i] = \"sadness\" \n",
    "   train_text[i][1] = [float(i) for i in lex[\"sadness\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 5:\n",
    "   data_scores.get(\"EMOT\")[i] = \"disgust\" \n",
    "   train_text[i][1] = [float(i) for i in lex[\"disgust\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 6:\n",
    "   data_scores.get(\"EMOT\")[i] = \"shame\" \n",
    "   train_text[i][1] = [float(i) for i in lex[\"shame\"]]\n",
    "  if data_scores.get(\"EMOT\")[i] == 7:\n",
    "   data_scores.get(\"EMOT\")[i] = \"guilt\" \n",
    "   train_text[i][1] = [float(i) for i in lex[\"guilt\"]]\n",
    "\n",
    "# prepare Goemotion data to format\n",
    "isear_data = []\n",
    "isear_v = []\n",
    "isear_a = []\n",
    "isear_d = []\n",
    "isear_vad = []\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "  isear_data.append(train_text[i][0])\n",
    "  isear_v.append(train_text[i][1][0])\n",
    "  isear_a.append(train_text[i][1][1])\n",
    "  isear_d.append(train_text[i][1][2])\n",
    "  isear_vad.append([train_text[i][1][0],train_text[i][1][1],train_text[i][1][2]])\n",
    "\n",
    "isear_data = np.array(isear_data)\n",
    "isear_v = np.array(isear_v)\n",
    "isear_a = np.array(isear_a)\n",
    "isear_d = np.array(isear_d)\n",
    "isear_vad = np.array(isear_vad)\n",
    "\n",
    "print(isear_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOvjrN2leLIo"
   },
   "source": [
    "Crowdflower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV0GwjcbeHac",
    "outputId": "1c280722-2697-4e5a-dec1-c3fbf45e13f8"
   },
   "outputs": [],
   "source": [
    "lexicon_path = \"content/Emotion-Detection-Datasets/NRC-VAD-Lexicon.txt\"\n",
    "lex = {}\n",
    "\n",
    "with open(lexicon_path) as f:\n",
    "  for line in f:\n",
    "    words = line.split()\n",
    "    try:\n",
    "      lex[words[0]] = [float(words[1]),float(words[2]),float(words[3])]\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "# path = \"content/Emotion-Detection-Datasets/isear.csv\"\n",
    "# data_text = pd.read_csv(path)\n",
    "# print(data_text.columns)\n",
    "\n",
    "raw_data = pd.read_csv('content/Emotion-Detection-Datasets/crowdflower.csv',on_bad_lines='skip')\n",
    "data_text = raw_data[[\"content\"]]\n",
    "data_scores = raw_data[[\"sentiment\"]]\n",
    "\n",
    "train_text = []\n",
    "\n",
    "for entry in data_text.get(\"content\"):\n",
    "    train_text.append(entry)\n",
    "for i in range(len(train_text)):\n",
    "  train_text[i] = [train_text[i],[0,0,0]]\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "\n",
    "   train_text[i][1] = [float(i) for i in lex[ data_scores.get(\"sentiment\")[i]]]\n",
    "\n",
    "# prepare Goemotion data to format\n",
    "cf_data = []\n",
    "cf_v = []\n",
    "cf_a = []\n",
    "cf_d = []\n",
    "cf_vad = []\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "  cf_data.append(train_text[i][0])\n",
    "  cf_v.append(train_text[i][1][0])\n",
    "  cf_a.append(train_text[i][1][1])\n",
    "  cf_d.append(train_text[i][1][2])\n",
    "  cf_vad.append([train_text[i][1][0],train_text[i][1][1],train_text[i][1][2]])\n",
    "\n",
    "cf_data = np.array(cf_data)\n",
    "cf_v = np.array(cf_v)\n",
    "cf_a = np.array(cf_a)\n",
    "cf_d = np.array(cf_d)\n",
    "cf_vad = np.array(cf_vad)\n",
    "\n",
    "print(cf_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXs69KLxCtOc",
    "outputId": "8563663d-c1a5-4cd5-d2cf-d8645f691ec4"
   },
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "combined_vad = []\n",
    "\n",
    "combined_data.extend(go_data)\n",
    "combined_data.extend(emo_data)\n",
    "combined_data.extend(isear_data)\n",
    "combined_vad.extend(go_vad)\n",
    "combined_vad.extend(emo_vad)\n",
    "combined_vad.extend(isear_vad)\n",
    "\n",
    "print(\"Number of samples:\",len(combined_vad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh2kZhR3p3L-"
   },
   "source": [
    "#### Prepare Training Data into Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REPF1qHrFHwK"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, vad_train, vad_test = train_test_split(combined_data, combined_vad, test_size=0.02, shuffle= False)\n",
    "x_train, x_valid, vad_train, vad_valid = train_test_split(x_train, vad_train, test_size=0.2, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csqecNCzlOLk"
   },
   "source": [
    "#### Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCnbLP3AXLxz",
    "outputId": "9a44254d-6d5b-4edc-bb4e-e8530161505a"
   },
   "outputs": [],
   "source": [
    "def distribution(data,name):\n",
    "  nine = 0\n",
    "  eight = 0\n",
    "  seven = 0\n",
    "  six = 0\n",
    "  five = 0\n",
    "  four = 0\n",
    "  three = 0\n",
    "  two = 0\n",
    "  one = 0\n",
    "  zero = 0\n",
    "  abszero = 0\n",
    "  for i in range(len(data)):\n",
    "    if data[i] > 0.95:\n",
    "      nine += 1\n",
    "    elif data[i] > 0.85:\n",
    "      eight += 1\n",
    "    elif data[i] > 0.75:\n",
    "      seven += 1\n",
    "    elif data[i] > 0.65:\n",
    "      six += 1\n",
    "    elif data[i] > 0.55:\n",
    "      five += 1\n",
    "    elif data[i] > 0.45:\n",
    "      four += 1\n",
    "    elif data[i] > 0.35:\n",
    "      three += 1\n",
    "    elif data[i] > 0.25:\n",
    "      two += 1\n",
    "    elif data[i] > 0.15:\n",
    "      one += 1\n",
    "    elif data[i] > 0.5:\n",
    "      zero += 1\n",
    "    else:\n",
    "      abszero += 1\n",
    "\n",
    "  max_value = max([abszero,zero,one,two,three,four,five,six,seven,eight,nine])\n",
    "  max_value = max_value*1.1\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  ax.bar([1,2,3,4,5,6,7,8,9,10,11], [abszero,zero,one,two,three,four,five,six,seven,eight,nine], width=.5, edgecolor=\"white\", linewidth=.5,tick_label=[\"0-0.5\",\"0.5-0.15\",\"0.15-0.25\",\"0.25-0.35\",\"0.35-0.45\",\"0.45-0.55\",\"0.55-0.65\",\"0.65-0.75\",\"0.75-0.85\",\"0.85-0.95\",\"0.95-1\"])\n",
    "\n",
    "  ax.set(xlim=(0, 12), xticks=np.arange(1, 12),\n",
    "        ylim=(0,max_value), yticks=np.arange(0,max_value,max_value-1))\n",
    "  ax.set_title(name)\n",
    "  plt.xticks(rotation='vertical')\n",
    "  plt.savefig(name,transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "distribution(np.array(go_vad).T[0],\"GoEmotion-Valence\")\n",
    "distribution(np.array(go_vad).T[1],\"GoEmotion-Arousal\")\n",
    "distribution(np.array(go_vad).T[2],\"GoEmotion-Dominance\")\n",
    "\n",
    "distribution(np.array(emo_vad).T[0],\"EmoBank-Valence\")\n",
    "distribution(np.array(emo_vad).T[1],\"EmoBank-Arousal\")\n",
    "distribution(np.array(emo_vad).T[2],\"EmoBank-Dominance\")\n",
    "\n",
    "distribution(np.array(isear_vad).T[0],\"ISEAR-Valence\")\n",
    "distribution(np.array(isear_vad).T[1],\"ISEAR-Arousal\")\n",
    "distribution(np.array(isear_vad).T[2],\"ISEAR-Dominance\")\n",
    "\n",
    "distribution(np.array(cf_vad).T[0],\"CrowdFlower-Valence\")\n",
    "distribution(np.array(cf_vad).T[1],\"CrowdFlower-Arousal\")\n",
    "distribution(np.array(cf_vad).T[2],\"CrowdFlower-Dominance\")\n",
    "\n",
    "distribution(np.array(combined_vad).T[0],\"Combined-Valence\")\n",
    "distribution(np.array(combined_vad).T[1],\"Combined-Arousal\")\n",
    "distribution(np.array(combined_vad).T[2],\"Combined-Dominance\")\n",
    "\n",
    "plt.scatter(np.array(combined_vad).T[0], np.array(combined_vad).T[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBuV5j2cS_b",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load a previously finetuned model here ...\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "classifier_model_vad = tf.keras.models.load_model(\"content/Emotion-Detection-Datasets/all_albertx20.h5\",custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZZiL08NfRK3"
   },
   "source": [
    "### ... or train anew/continue training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "#### Define your model or load a previously trained model\n",
    "\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
    "\n",
    "Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8_ctG55-uTX",
    "outputId": "94a8f15b-9390-4469-a5af-29da89d7c8b0"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'albert_en_base'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "LzVKH-ZVVlNR",
    "outputId": "9caef6ea-9593-4cf4-e0cd-4a6523494681"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model_vad():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  # net = tf.keras.layers.Dense(3, activation=None, name='classifier')(net)\n",
    "  net = tf.keras.layers.Dense(3)(net)\n",
    "\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model_vad = build_classifier_model_vad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "#### Optimizer\n",
    "\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "metrics = tf.metrics.mean_squared_error\n",
    "\n",
    "epochs = 5\n",
    "# steps_per_epoch = tf.data.experimental.cardinality(x_train).numpy()\n",
    "print(len(x_train))\n",
    "steps_per_epoch = len(x_train)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "#### Compiling the BERT model, training and saving\n",
    "\n",
    "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model_vad.compile(optimizer=optimizer,loss=loss,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "history_vad = classifier_model_vad.fit(x=x_train,y=vad_train,validation_data=(x_valid,vad_valid),epochs=epochs)\n",
    "classifier_model_vad.save(\"all_Albert\" + \"/vad.h5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9a5nunJj4e9"
   },
   "outputs": [],
   "source": [
    "classifier_model_vad.save(\"model\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NGQC8Zck8BB"
   },
   "outputs": [],
   "source": [
    "!zip -r model.zip model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, mean_squared_error = classifier_model_vad.evaluate(x=x_valid,y=vad_valid)\n",
    "print(\"VAD: \")\n",
    "print(f'Loss: {loss}')\n",
    "print(f'mean squared error: {mean_squared_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "#### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6I-5mdJg5Ca"
   },
   "outputs": [],
   "source": [
    "print(classifier_model_vad.history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history_vad.history\n",
    "# history_dict = {\"mean_squared_error\":[ 0.0225,0.0183,0.0163,0.0150,0.0113]}\n",
    "\n",
    "acc = history_dict['mean_squared_error']\n",
    "val_acc = history_dict['mean_squared_error']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# plt.plot(epochs, acc, 'r', label='Training mean_squared_error')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation mean_squared_error')\n",
    "plt.title('Validation mean_squared_error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('mean_squared_error')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"Training\",transparent = True,bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur9qtVA82YVN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rule-based Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F7B-YPVeioq"
   },
   "outputs": [],
   "source": [
    "# create NRC-VAD Lexicon\n",
    "lexicon_path = \"content/Emotion-Detection-Datasets/NRC-VAD-Lexicon.txt\"\n",
    "lex = {}\n",
    "\n",
    "with open(lexicon_path) as f:\n",
    "  for line in f:\n",
    "    words = line.split()\n",
    "    try:\n",
    "      lex[words[0]] = [float(words[1]),float(words[2]),float(words[3])]\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "\n",
    "# transform POS to wordnet scheme\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return \"a\"\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return \"v\"\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return \"n\"\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "# negation function for vad scores\n",
    "def negate(score):\n",
    "\n",
    "  for i in range(len(score)):\n",
    "    difference = abs(score[i]-.5)\n",
    "    if score[i] < .5:\n",
    "      score[i] += difference*2\n",
    "    else:\n",
    "      score[i] -= difference*2\n",
    "  return score\n",
    "\n",
    "# Remove accents function\n",
    "def remove_accents(data):\n",
    "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
    "\n",
    "# Rule based ED\n",
    "def emotion_detection(input):\n",
    "\n",
    "  # transform string into list\n",
    "  input = input.split()\n",
    "\n",
    "  stopwords = nltk.corpus.stopwords.words('english')\n",
    "  stemmer = nltk.stem.PorterStemmer()\n",
    "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "  # remove accents and punctuation \n",
    "  input = [remove_accents(x) for x in input]\n",
    "\n",
    "  # transform to lowercase\n",
    "  input = [x.lower() for x in input]\n",
    "\n",
    "  pos = nltk.pos_tag(input)\n",
    "\n",
    "  for i in range(len(input)):\n",
    "    input[i] = lemmatizer.lemmatize(input[i],get_wordnet_pos(pos[i][1]))\n",
    "\n",
    "  score = [0,0,0]\n",
    "  total = 0\n",
    "\n",
    "  for i in range(len(input)):\n",
    "\n",
    "\n",
    "    if input[i] in lex:\n",
    "      \n",
    "      if input[i-1] == (\"not\" or \"never\"):\n",
    "        score = np.add(score,negate([float(i) for i in lex[input[i]]]))\n",
    "      else:\n",
    "        score = np.add(score,[float(i) for i in lex[input[i]]])\n",
    "      total += 1\n",
    "\n",
    "  if total > 0:\n",
    "    score = [float(x)/total for x in score]\n",
    "\n",
    "  return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbWmkP-F1Z18"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsJnICGJJea9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helperfunctions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b7lZlocYvgL"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_combined(real,target):\n",
    "  error=0\n",
    "  for i in range(len(real)):\n",
    "    error += abs(real[i]-target[i])\n",
    "  return (error/len(real))**2\n",
    "\n",
    "def mean_squared_error_individual(real,target):\n",
    "  error = abs(real-target)\n",
    "  return error**2\n",
    "\n",
    "def map_to_categories_vad(vad_score,categories):\n",
    "  vad_categories = {}\n",
    "  differences = {}\n",
    "  for category in categories:\n",
    "    vad_categories[category] = lex[category]\n",
    "  for category in vad_categories.keys():\n",
    "    differences[category] = np.absolute(np.subtract(vad_categories[category],vad_score))\n",
    "    mean = 0\n",
    "    for x in differences[category]:\n",
    "      mean += x\n",
    "    mean = mean/len(differences[category])\n",
    "    differences[category] = mean\n",
    "  return (min(differences, key=differences.get))\n",
    "\n",
    "def map_to_categories_va(vad_score,categories):\n",
    "  vad_categories = {}\n",
    "  differences = {}\n",
    "  for category in categories:\n",
    "    vad_categories[category] = lex[category][:-1]\n",
    "  for category in vad_categories.keys():\n",
    "    differences[category] = np.absolute(np.subtract(vad_categories[category],vad_score[:-1]))\n",
    "    mean = 0\n",
    "    for x in differences[category]:\n",
    "      mean += x\n",
    "    mean = mean/len(differences[category])\n",
    "    differences[category] = mean\n",
    "  return (min(differences, key=differences.get))\n",
    "\n",
    "\n",
    "def evaluate(data,vad):\n",
    "\n",
    "  # import time\n",
    "  # start = time.time()\n",
    "\n",
    "  mse_v = 0\n",
    "  mse_a = 0\n",
    "  mse_d = 0\n",
    "  mse_combined = 0\n",
    "  ml_predicted = []\n",
    "  for i in range(len(data)):\n",
    "    predicted = classifier_model_vad(tf.constant([data[i]])).numpy()\n",
    "    mse_v += mean_squared_error_individual(predicted[0][0],vad[i][0])\n",
    "    mse_a += mean_squared_error_individual(predicted[0][1],vad[i][1])\n",
    "    mse_d += mean_squared_error_individual(predicted[0][2],vad[i][2])\n",
    "    mse_combined += mean_squared_error_combined(predicted[0],vad[i])\n",
    "    ml_predicted.append(predicted[0])\n",
    "  mse_combined_v = mse_v/len(data)\n",
    "  mse_combined_a = mse_a/len(data)\n",
    "  mse_combined_d = mse_d/len(data)\n",
    "  mse_combined_total = mse_combined/len(data)\n",
    "  coefficient_v = np.corrcoef(np.array(ml_predicted).T[0],np.array(vad).T[0])[0][1]\n",
    "  coefficient_a = np.corrcoef(np.array(ml_predicted).T[1],np.array(vad).T[1])[0][1]\n",
    "  coefficient_d = np.corrcoef(np.array(ml_predicted).T[2],np.array(vad).T[2])[0][1]\n",
    "  coefficient_vad = (coefficient_v+coefficient_a+coefficient_d)/3\n",
    "\n",
    "\n",
    "  print(\"V MSE ML-approach:\",mse_combined_v)\n",
    "  print(\"A MSE ML-approach:\",mse_combined_a)\n",
    "  print(\"D MSE ML-approach:\",mse_combined_d)\n",
    "  print(\"Combined MSE ML-approach:\",mse_combined_total)\n",
    "  print(\"V correlation ml\",coefficient_v)\n",
    "  print(\"A correlation ml\",coefficient_a)\n",
    "  print(\"D correlation ml\",coefficient_d)\n",
    "  print(\"Combined correlation ML-approach:\",coefficient_vad)  \n",
    "\n",
    "  mse_v = 0\n",
    "  mse_a = 0\n",
    "  mse_d = 0\n",
    "  mse_combined = 0\n",
    "  r_predicted = []\n",
    "  for i in range(len(data)):\n",
    "    predicted = emotion_detection(data[i])\n",
    "    mse_v += mean_squared_error_individual(predicted[0],vad[i][0])\n",
    "    mse_a += mean_squared_error_individual(predicted[1],vad[i][1])\n",
    "    mse_d += mean_squared_error_individual(predicted[2],vad[i][2])\n",
    "    mse_combined += mean_squared_error_combined(predicted,vad[i])\n",
    "    r_predicted.append(predicted)\n",
    "  mse_combined_v = mse_v/len(data)\n",
    "  mse_combined_a = mse_a/len(data)\n",
    "  mse_combined_d = mse_d/len(data)\n",
    "  mse_combined_total = mse_combined/len(data)\n",
    "\n",
    "  # end = time.time()\n",
    "  # print(end-start)\n",
    "\n",
    "  coefficient_v = np.corrcoef(np.array(r_predicted).T[0],np.array(vad).T[0])[0][1]\n",
    "  coefficient_a = np.corrcoef(np.array(r_predicted).T[1],np.array(vad).T[1])[0][1]\n",
    "  coefficient_d = np.corrcoef(np.array(r_predicted).T[2],np.array(vad).T[2])[0][1]\n",
    "  coefficient_vad = (coefficient_v+coefficient_a+coefficient_d)/3\n",
    "\n",
    "  print(\"V MSE Rule-approach:\",mse_combined_v)\n",
    "  print(\"A MSE Rule-approach:\",mse_combined_a)\n",
    "  print(\"D MSE Rule-approach:\",mse_combined_d)\n",
    "  print(\"Combined MSE Rule-approach:\",mse_combined_total)\n",
    "  print(\"V correlation rule\",coefficient_v)\n",
    "  print(\"A correlation rule\",coefficient_a)\n",
    "  print(\"D correlation rule\",coefficient_d)\n",
    "  print(\"Combined correlation Rule-approach:\",coefficient_vad)\n",
    "  print(\"\\n\")\n",
    "\n",
    "def visualize_evaluation(data,vad):\n",
    "\n",
    "  ml_predicted = []\n",
    "  r_predicted = []\n",
    "  for i in range(len(data)):\n",
    "    predicted_ml = classifier_model_vad(tf.constant([data[i]])).numpy()\n",
    "    predicted_r = emotion_detection(data[i])\n",
    "    ml_predicted.append(predicted_ml[0])\n",
    "    r_predicted.append(predicted_r)\n",
    "\n",
    "  v = [i[0] for i in vad]\n",
    "  a = [i[1] for i in vad]\n",
    "  d = [i[2] for i in vad]\n",
    "\n",
    "  print(\"Real Data distribution\")\n",
    "\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Arousal')\n",
    "  plt.scatter(v, a)\n",
    "  plt.savefig(\"VA-Real\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(v, d)\n",
    "  plt.savefig(\"VD-Real\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  plt.xlabel('Arousal')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(a, d)\n",
    "  plt.savefig(\"AD-Real\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  v = [word[0] for word in ml_predicted]\n",
    "  a = [word[1] for word in ml_predicted]\n",
    "  d = [word[2] for word in ml_predicted]\n",
    "\n",
    "  print(\"ML-inferred Data distribution\")\n",
    "\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Arousal')\n",
    "  plt.scatter(v, a)\n",
    "  plt.savefig(\"VA-ML\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(v, d)\n",
    "  plt.savefig(\"VD-ML\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  plt.xlabel('Arousal')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(a, d)\n",
    "  plt.savefig(\"AD-ML\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "  v = [word[0] for word in r_predicted]\n",
    "  a = [word[1] for word in r_predicted]\n",
    "  d = [word[2] for word in r_predicted]\n",
    "\n",
    "  print(\"Rule-inferred Data distribution\")\n",
    "\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Arousal')\n",
    "  plt.scatter(v, a)\n",
    "  plt.savefig(\"VA-Rule\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "  plt.xlabel('Valence')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(v, d)\n",
    "  plt.savefig(\"VD-Rule\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "  plt.xlabel('Arousal')\n",
    "  plt.ylabel('Dominance')\n",
    "  plt.scatter(a, d)\n",
    "  plt.savefig(\"AD-Rule\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "def evaluate_categorical_ml(data,vad):\n",
    "\n",
    "  categories = {\"empty\":[0,0,0,0,0,0,0],\"threatened\":[0,0,0,0,0,0,0],\"tranquil\":[0,0,0,0,0,0,0],\"excited\":[0,0,0,0,0,0,0],\"rooted\":[0,0,0,0,0,0,0]}\n",
    "  tp = 0\n",
    "  total = 0\n",
    "  dataset_distribution_categorical = {}\n",
    "  for key in categories.keys():\n",
    "    dataset_distribution_categorical[key]=0\n",
    "\n",
    "  metrics = {}\n",
    "  for category in categories:\n",
    "    metrics[category] = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\"Precision\":0,\"Recall\":0,\"F1\":0}\n",
    "\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    prediction = map_to_categories_va(classifier_model_vad(tf.constant([data[i]])).numpy()[0],categories.keys())\n",
    "    real = map_to_categories_va(vad[i],categories.keys())\n",
    "    dataset_distribution_categorical[real] += 1\n",
    "    if prediction == real:\n",
    "      tp +=1\n",
    "      total += 1\n",
    "      for category in metrics.keys():\n",
    "        if prediction == category:\n",
    "          metrics[category][\"TP\"] += 1\n",
    "        else:\n",
    "          metrics[category][\"TN\"] += 1\n",
    "\n",
    "          \n",
    "    else:\n",
    "      total += 1\n",
    "      for category in metrics.keys():\n",
    "        if prediction == category:\n",
    "          metrics[category][\"FP\"] += 1\n",
    "        elif real == category:\n",
    "          metrics[category][\"FN\"] += 1\n",
    "        else:         \n",
    "          metrics[category][\"TN\"] += 1\n",
    "\n",
    "  for category in metrics.keys():\n",
    "    if metrics[category][\"TP\"]==0:\n",
    "      metrics[category][\"Precision\"] = 0\n",
    "      metrics[category][\"Recall\"] = 0\n",
    "      metrics[category][\"F1\"] = 0\n",
    "    else:\n",
    "      metrics[category][\"Precision\"] = metrics[category][\"TP\"]/(metrics[category][\"TP\"]+metrics[category][\"FP\"])\n",
    "      metrics[category][\"Recall\"] = metrics[category][\"TP\"]/(metrics[category][\"TP\"]+metrics[category][\"FN\"])\n",
    "      metrics[category][\"F1\"] = 2*metrics[category][\"Precision\"]*metrics[category][\"Recall\"]/(metrics[category][\"Precision\"]+metrics[category][\"Recall\"])\n",
    "\n",
    "  macro = 0\n",
    "  total_tp = 0\n",
    "  total_fp = 0\n",
    "  total_fn = 0\n",
    "  average = 0\n",
    "\n",
    "  for category in metrics.keys():\n",
    "    macro +=   metrics[category][\"F1\"]\n",
    "    total_tp += metrics[category][\"TP\"]\n",
    "    total_fp += metrics[category][\"FP\"]\n",
    "    total_fn += metrics[category][\"FN\"]\n",
    "\n",
    "  macro = macro/len(metrics.keys())\n",
    "  micro = total_tp/(total_tp+.5*(total_fp+total_fn))\n",
    "  for category in categories:\n",
    "    average += (dataset_distribution_categorical[category]/total)*metrics[category][\"F1\"]\n",
    "  print(metrics)\n",
    "  print(\"accuracy =\",tp/total)\n",
    "  print(\"macro f1 =\",macro)\n",
    "  print(\"micro f1 =\",micro)\n",
    "  print(\"average f1 =\",average)\n",
    "\n",
    "def evaluate_categorical_r(data,vad):\n",
    "\n",
    "  categories = {\"empty\":[0,0,0,0,0,0,0],\"threatened\":[0,0,0,0,0,0,0],\"tranquil\":[0,0,0,0,0,0,0],\"excited\":[0,0,0,0,0,0,0],\"rooted\":[0,0,0,0,0,0,0]}\n",
    "  tp = 0\n",
    "  total = 0\n",
    "  dataset_distribution_categorical = {}\n",
    "  for key in categories.keys():\n",
    "    dataset_distribution_categorical[key]=0\n",
    "\n",
    "  metrics = {}\n",
    "  for category in categories:\n",
    "    metrics[category] = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\"Precision\":0,\"Recall\":0,\"F1\":0}\n",
    "\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    prediction = map_to_categories_va(emotion_detection(data[i]),categories.keys())\n",
    "    real = map_to_categories_va(vad[i],categories.keys())\n",
    "    dataset_distribution_categorical[real] += 1\n",
    "    if prediction == real:\n",
    "      tp +=1\n",
    "      total += 1\n",
    "      for category in metrics.keys():\n",
    "        if prediction == category:\n",
    "          metrics[category][\"TP\"] += 1\n",
    "        else:\n",
    "          metrics[category][\"TN\"] += 1\n",
    "\n",
    "          \n",
    "    else:\n",
    "      total += 1\n",
    "      for category in metrics.keys():\n",
    "        if prediction == category:\n",
    "          metrics[category][\"FP\"] += 1\n",
    "        elif real == category:\n",
    "          metrics[category][\"FN\"] += 1\n",
    "        else:         \n",
    "          metrics[category][\"TN\"] += 1\n",
    "\n",
    "  for category in metrics.keys():\n",
    "    if metrics[category][\"TP\"]==0:\n",
    "      metrics[category][\"Precision\"] = 0\n",
    "      metrics[category][\"Recall\"] = 0\n",
    "      metrics[category][\"F1\"] = 0\n",
    "    else:\n",
    "      metrics[category][\"Precision\"] = metrics[category][\"TP\"]/(metrics[category][\"TP\"]+metrics[category][\"FP\"])\n",
    "      metrics[category][\"Recall\"] = metrics[category][\"TP\"]/(metrics[category][\"TP\"]+metrics[category][\"FN\"])\n",
    "      metrics[category][\"F1\"] = 2*metrics[category][\"Precision\"]*metrics[category][\"Recall\"]/(metrics[category][\"Precision\"]+metrics[category][\"Recall\"])\n",
    "\n",
    "  macro = 0\n",
    "  total_tp = 0\n",
    "  total_fp = 0\n",
    "  total_fn = 0\n",
    "  average = 0\n",
    "\n",
    "  for category in metrics.keys():\n",
    "    macro +=   metrics[category][\"F1\"]\n",
    "    total_tp += metrics[category][\"TP\"]\n",
    "    total_fp += metrics[category][\"FP\"]\n",
    "    total_fn += metrics[category][\"FN\"]\n",
    "\n",
    "  macro = macro/len(metrics.keys())\n",
    "  micro = total_tp/(total_tp+.5*(total_fp+total_fn))\n",
    "  for category in categories:\n",
    "    average += (dataset_distribution_categorical[category]/total)*metrics[category][\"F1\"]\n",
    "  print(metrics)\n",
    "  print(\"accuracy =\",tp/total)\n",
    "  print(\"macro f1 =\",macro)\n",
    "  print(\"micro f1 =\",micro)\n",
    "  print(\"average f1 =\",average)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYiTw2UrVNHB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dimensional Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHciFHD-is84",
    "outputId": "e6ad5249-6aac-4f82-ca9b-6dee612c5548"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluation against Emobank:\")\n",
    "evaluate(emo_data,emo_vad)\n",
    "\n",
    "print(\"Evaluation against GoEmotion:\")\n",
    "evaluate(go_data,go_vad)\n",
    "\n",
    "print(\"Evaluation against ISEAR:\")\n",
    "evaluate(isear_data,isear_vad)\n",
    "\n",
    "print(\"Evaluation against Combined:\")\n",
    "evaluate(combined_data,combined_vad)\n",
    "\n",
    "print(\"Evaluation against Split:\")\n",
    "evaluate(x_test,vad_test)\n",
    "\n",
    "print(\"Evaluation against CrowdFlower:\")\n",
    "evaluate(cf_data,cf_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vryXyCaiomaP"
   },
   "source": [
    "#### Visualize Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Tt1ZW97T1XE",
    "outputId": "ce36878e-362b-4c72-d5ef-860f173887d4"
   },
   "outputs": [],
   "source": [
    "visualize_evaluation(emo_data,emo_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1daW_Jok3r7O"
   },
   "source": [
    "### Categorical Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFqjB65J33by",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Visualize Transformation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "U8w9Fzl3tMHc",
    "outputId": "78526768-4bff-4233-d33a-2dd083aaa013"
   },
   "outputs": [],
   "source": [
    "# scheme: emotion: [tp,tn,fp,fn,precision,recall,f1]\n",
    "categories = {\"empty\":[0,0,0,0,0,0,0],\"threatened\":[0,0,0,0,0,0,0],\"tranquil\":[0,0,0,0,0,0,0],\"excited\":[0,0,0,0,0,0,0],\"rooted\":[0,0,0,0,0,0,0]}\n",
    "v = []\n",
    "a = []\n",
    "colors = [\"blue\",\"red\",\"green\",\"orange\",\"pink\"]\n",
    "\n",
    "for entry in categories:\n",
    "  v.append(lex[entry][0])\n",
    "  a.append(lex[entry][1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "\n",
    "ax.annotate(\"empty\", (v[0], a[0]),xytext=(0.22, 0.16))\n",
    "ax.annotate(\"threatened\", (v[1], a[1]),xytext=(0.1, 0.9))\n",
    "ax.annotate(\"tranquil\", (v[2], a[2]),xytext=(0.77, 0.08))\n",
    "ax.annotate(\"excited\", (v[3], a[3]),xytext=(0.77, 0.9))\n",
    "ax.annotate(\"rooted\", (v[4], a[4]),xytext=(0.55, 0.5))\n",
    "\n",
    "\n",
    "for i in range(len(colors)):\n",
    "  plt.scatter(v[i],a[i],color = colors[i])\n",
    "\n",
    "plt.savefig(\"Categorization\",transparent = True,bbox_inches='tight',dpi=300)\n",
    "plt.show()\n",
    "\n",
    "colors = {\"empty\":\"blue\",\"threatened\":\"red\",\"tranquil\":\"green\",\"excited\":\"orange\",\"rooted\":\"pink\"}\n",
    "v = []\n",
    "a = []\n",
    "c = []\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "  \n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "\n",
    "score = []\n",
    "units = 30\n",
    "for i in range(units):\n",
    "  for j in range(units):\n",
    "    score.append([[i/units,j/units,1],colors[map_to_categories_va([i/units,j/units,1],categories.keys())]])\n",
    "\n",
    "\n",
    "for i in range(len(score)):\n",
    "  # print(score[i][0][0],score[i][0][1],score[i][1])\n",
    "  plt.scatter(score[i][0][0],score[i][0][1],color = score[i][1])\n",
    "\n",
    "plt.savefig(\"Categorization_distance\",transparent = True,bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGx8LJ63KkmM",
    "outputId": "3d580a49-ae8a-4241-b713-b76dc0997468"
   },
   "outputs": [],
   "source": [
    "dataset_distribution_categorical = {}\n",
    "total = 0\n",
    "\n",
    "for key in categories.keys():\n",
    "  dataset_distribution_categorical[key]=0\n",
    "\n",
    "for datapoint in combined_vad:\n",
    "  dataset_distribution_categorical[map_to_categories_va(datapoint,categories.keys())] += 1\n",
    "  total += 1\n",
    "\n",
    "for key in categories.keys():\n",
    "  dataset_distribution_categorical[key]=dataset_distribution_categorical[key]/total\n",
    "\n",
    "print(\"Categorical Distribution of the train dataset:\", dataset_distribution_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_hFoo24MUd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Categorical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cxoypDCimga",
    "outputId": "d4ea1545-67f6-4fb8-9db1-2d408b176277"
   },
   "outputs": [],
   "source": [
    "# print(\"Combined:\")\n",
    "# evaluate_categorical_ml(combined_data,combined_vad)\n",
    "# evaluate_categorical_r(combined_data,combined_vad)\n",
    "# print(\"CrowdFlower:\")\n",
    "# evaluate_categorical_ml(cf_data,cf_vad)\n",
    "# evaluate_categorical_r(cf_data,cf_vad)\n",
    "print(\"GoEmotion:\")\n",
    "evaluate_categorical_ml(go_data,go_vad)\n",
    "evaluate_categorical_r(go_data,go_vad)\n",
    "print(\"ISEAR:\")\n",
    "evaluate_categorical_ml(isear_data,isear_vad)\n",
    "evaluate_categorical_r(isear_data,isear_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fUCQnbdk-Ga"
   },
   "source": [
    "### Infer example sentences in both approaches"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26IHvVJHkvd1",
    "outputId": "aa2cda8f-0d5c-4774-f1c2-db86ffb886a5"
   },
   "source": [
    "def print_my_examples_ml(inputs, results):\n",
    "  categories = {\"empty\":[0,0,0,0,0,0,0],\"threatened\":[0,0,0,0,0,0,0],\"tranquil\":[0,0,0,0,0,0,0],\"excited\":[0,0,0,0,0,0,0],\"rooted\":[0,0,0,0,0,0,0]}\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : Valence: {results[i][0]:.6f}, Arousal: {results[i][1]:.6f}, Dominance: {results[i][2]:.6f}, Category: {map_to_categories_va([results[i][0],results[i][1]],categories)}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "\n",
    "def print_my_examples_r(inputs, results):\n",
    "  categories = {\"empty\":[0,0,0,0,0,0,0],\"threatened\":[0,0,0,0,0,0,0],\"tranquil\":[0,0,0,0,0,0,0],\"excited\":[0,0,0,0,0,0,0],\"rooted\":[0,0,0,0,0,0,0]}\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : Valence: {results[i][0]:.6f}, Arousal: {results[i][1]:.6f}, Dominance: {results[i][2]:.6f}, Category: {map_to_categories_va([results[i][0],results[i][1]],categories)}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'I am happy',\n",
    "    'I am sad',\n",
    "    'I am not happy',\n",
    "    'I feel not very bad.',\n",
    "    'I feel absolutely amazing!',\n",
    "    'I want to kill myself!',\n",
    "    'Never have I been happier than today!',\n",
    "    'I will never be happy!',\n",
    "    'I will end your life!',\n",
    "    'You the fucking best!'\n",
    "]\n",
    "\n",
    "print('ML-based:')\n",
    "print_my_examples_ml(examples, classifier_model_vad(tf.constant(examples)))\n",
    "print('Rule-based:')\n",
    "print_my_examples_r(examples, [emotion_detection(example) for example in examples])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "j3v6-Uk62HyA",
    "EtRV0thhLrFT",
    "lZZiL08NfRK3",
    "77psrpfzbxtp",
    "Ur9qtVA82YVN",
    "uYiTw2UrVNHB",
    "vryXyCaiomaP"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0f44f747889f381a4982e2d430fad45f24226566a183968fdef54143fb11c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
